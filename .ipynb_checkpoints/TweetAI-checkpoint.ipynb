{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "GOAL: To be able to take in a group of tweets, and generate new tweets based on the input\n",
    "\n",
    "WHY: This system is flexible and works with any twitter user or any group of tweets.\n",
    "     So ultimately can be applicable and mimicking someone's voice or learning a character.\n",
    "     \n",
    "HOW: An LSTM where the X is a group of characters and the y is the following character\n",
    "     So rather than word by word, the system learns to type character by character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "\n",
    "auth = tweepy.OAuthHandler('OX6EZZ9kea7e3QgICtbN5fep0',\n",
    "                           'F5IYMC7pEUo4ntf2mSU9vAuO4Z4tGa3pb2yKgnxR4cEZItJrWa')\n",
    "auth.set_access_token('360249871-fMMBIO5myYfsFfDE35FYak1EU1p3rACWCBP0BZga',\n",
    "                      'kqZz9dzHEPsMkEZWXCbWlucdW6bYAoP0nYTh6g5QI5vkC')\n",
    "\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most recently fetched tweet:  Great to be in Riyadh, Saudi Arabia. Looking forward to the afternoon and evening ahead. #POTUSAbroad https://t.co/JJOra0KfyR\n",
      "getting tweets before 865865814099939327\n",
      "...%s tweets downloaded so far400\n",
      "Most recently fetched tweet:  I will be interviewed by @MariaBartiromo at 6:00 A.M. @FoxBusiness. Enjoy!\n",
      "getting tweets before 851997299454742527\n",
      "...%s tweets downloaded so far600\n",
      "Most recently fetched tweet:  Going to CPAC!\n",
      "getting tweets before 835124485632180223\n",
      "...%s tweets downloaded so far800\n",
      "Most recently fetched tweet:  It all begins today! I will see you at 11:00 A.M. for the swearing-in. THE MOVEMENT CONTINUES - THE WORK BEGINS!\n",
      "getting tweets before 822421390125043712\n",
      "...%s tweets downloaded so far1000\n",
      "Most recently fetched tweet:  Well, we all did it, together! I hope the \"MOVEMENT\" fans will go to D.C. on Jan 20th for the swearing in. Let's set the all time record!\n",
      "getting tweets before 809790978332786688\n",
      "...%s tweets downloaded so far1199\n",
      "Most recently fetched tweet:  Thank you for you support Virginia! In ONE DAY - get out and #VoteTrumpPence16! #ICYMI: https://t.co/Nid8qcFTwY https://t.co/WOsEcjM8sm\n",
      "getting tweets before 795515037632921600\n",
      "...%s tweets downloaded so far1399\n",
      "Most recently fetched tweet:  #CrookedHillary sending U.S. intelligence info. to Podestaâ€™s hacked email is â€˜unquestionably an OPSEC violationâ€™ https://t.co/z58aeo4CO7\n",
      "getting tweets before 789599646821806084\n",
      "...%s tweets downloaded so far1599\n",
      "Most recently fetched tweet:  The phony story in the failing @nytimes is a TOTAL FABRICATION. Written by same people as last discredited story on women. WATCH!\n",
      "getting tweets before 786560925113266175\n",
      "...%s tweets downloaded so far1799\n",
      "Most recently fetched tweet:  The so-called Commission on Presidential Debates admitted to us that the DJT audio &amp; sound level was very bad. So why didn't they fix it?\n",
      "getting tweets before 782321125984436223\n",
      "...%s tweets downloaded so far1999\n",
      "Most recently fetched tweet:  I will be interviewed on @foxandfriends at 7:00 A.M.\n",
      "getting tweets before 775275158340853759\n",
      "...%s tweets downloaded so far2199\n",
      "Most recently fetched tweet:  \"@J58golf: @realDonaldTrump been a great week. More of the same will get you in the white house!\"\n",
      "getting tweets before 767149739418804227\n",
      "...%s tweets downloaded so far2399\n",
      "Most recently fetched tweet:  Violent crime is rising across the United States, yet the DNC convention ignored it. Crime reduction will be one of my top priorities.\n",
      "getting tweets before 759450053379579904\n",
      "...%s tweets downloaded so far2599\n",
      "Most recently fetched tweet:  It doesn't matter that Crooked Hillary has experience, look at all of the bad decisions she has made. Bernie said she has bad judgement!\n",
      "getting tweets before 754648419151511551\n",
      "...%s tweets downloaded so far2799\n",
      "Most recently fetched tweet:  Thoughts and prayers are with everyone in West Virginia- dealing with the devastating floods. #ImWithYou\n",
      "getting tweets before 746560976921706499\n",
      "...%s tweets downloaded so far2999\n",
      "Most recently fetched tweet:  Thank you Redding, California!\n",
      "#MakeAmericaGreatAgain #CAPrimary https://t.co/nFiydpTpPn\n",
      "getting tweets before 738858013419835391\n",
      "...%s tweets downloaded so far3199\n",
      "Most recently fetched tweet:  \"@jlund04: @megynkelly @realDonaldTrump it was refreshing to see you both in a different light. Well done.\"\n",
      "getting tweets before 732732606765240320\n",
      "...%s tweets downloaded so far3213\n",
      "Most recently fetched tweet:  I will be live tweeting @megynkelly Show in 10 minutes. Should be interesting. Will be on Fox Network! ENJOY!\n",
      "getting tweets before 732719796077383679\n",
      "...%s tweets downloaded so far3213\n",
      "Most recently fetched tweet:  I will be live tweeting @megynkelly Show in 10 minutes. Should be interesting. Will be on Fox Network! ENJOY!\n"
     ]
    }
   ],
   "source": [
    "#initialize a list to hold all the tweepy Tweets\n",
    "alltweets = []\n",
    "\n",
    "#make initial request for most recent tweets (200 is the maximum allowed count)\n",
    "new_tweets = api.user_timeline('realDonaldTrump',count=200)\n",
    "\n",
    "#save most recent tweets\n",
    "alltweets.extend(new_tweets)\n",
    "\n",
    "#save the id of the oldest tweet less one\n",
    "oldest = alltweets[-1].id - 1\n",
    "\n",
    "print(\"Most recently fetched tweet: \", alltweets[-1].text)\n",
    "\n",
    "#keep grabbing tweets until there are no tweets left to grab\n",
    "while len(new_tweets) > 0:\n",
    "    print(\"getting tweets before \" + str(oldest))\n",
    "    #all subsiquent requests use the max_id param to prevent duplicates\n",
    "    new_tweets = api.user_timeline('realDonaldTrump',count=200,max_id=oldest)\n",
    "\n",
    "    #save most recent tweets\n",
    "    alltweets.extend(new_tweets)\n",
    "\n",
    "    #update the id of the oldest tweet less one\n",
    "    oldest = alltweets[-1].id - 1\n",
    "    print(\"...%s tweets downloaded so far\" + str(len(alltweets)))\n",
    "    print(\"Most recently fetched tweet: \", alltweets[-1].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "non_retweets = [t.text for t in alltweets if 'RT @' not in t.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.@FLOTUS &amp; I were honored to host our first WH Congressional Picnic. A wonderful evening &amp; tradition. @MarineBand:â€¦ https://t.co/h5L4myWmam',\n",
       " \"We're all thinking of you @SteveScalise! #TeamScalise https://t.co/Yqf6exhm7x\",\n",
       " 'Together, we are going to MAKE AMERICA GREAT AGAIN!\\n#AmericaFirstðŸ‡ºðŸ‡¸ https://t.co/MVJbs44AtR',\n",
       " 'I am very supportive of the Senate #HealthcareBill. Look forward to making it really special! Remember, ObamaCare is dead.',\n",
       " 'Mexico was just ranked the second deadliest country in the world, after only Syria. Drug trade is largely the cause. We will BUILD THE WALL!',\n",
       " 'As promised, our campaign against the MS-13 gang continues. \\n\\n\"@ICEgov Busts 39 MS-13 Members in New York Operation\"\\nhttps://t.co/ki41GXeCMy',\n",
       " '...whether there are \"tapes\" or recordings of my conversations with James Comey, but I did not make, and do not have, any such recordings.',\n",
       " 'With all of the recently reported electronic surveillance, intercepts, unmasking and illegal leaking of information, I have no idea...',\n",
       " \"I certainly hope the Democrats do not force Nancy P out. That would be very bad for the Republican Party - and please let Cryin' Chuck stay!\",\n",
       " \"...Why did the DNC REFUSE to turn over its Server to the FBI, and still hasn't? It's all a big Dem scam and excuse for losing the election!\"]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_retweets[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# REMOVING LINKS, EMOJIS, AND OTHER EXTRANEOUS CHARACTERS\n",
    "\n",
    "non_links = []\n",
    "for tweet in non_retweets:\n",
    "    new_tweet = ' '.join(word for word in tweet.split() if word[0:4] != 'http')\n",
    "    non_links.append(new_tweet)\n",
    "    \n",
    "# remove extraneous links\n",
    "for tweet in non_links:\n",
    "    if 'http' in tweet:\n",
    "        non_links.remove(tweet)\n",
    "        \n",
    "import string\n",
    "# removes all emojis from tweets\n",
    "# technically removes all characters that are not letter, numbers, punctuation, or whitespace\n",
    "no_emojis = []\n",
    "for tweet in non_links:\n",
    "    for c in list(tweet):\n",
    "        if not c.isalnum() and c not in string.punctuation and c not in string.whitespace:\n",
    "            tweet = tweet.replace(c, '')\n",
    "    no_emojis.append(tweet)\n",
    "    \n",
    "# changes '&amp;' to simply '&'\n",
    "# NEW - maybe replace with 'and' so it will learn how to spell 'and' rather have to learn where to place &\n",
    "# idk bru\n",
    "final_tweets = []\n",
    "for tweet in no_emojis:\n",
    "    if '&amp;' in tweet:\n",
    "        tweet = tweet.replace('&amp;', '&')\n",
    "    final_tweets.append(tweet)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# remove extraneous links\n",
    "for tweet in non_links:\n",
    "    if 'http' in tweet:\n",
    "        non_links.remove(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2983"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(non_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "# removes all emojis from tweets\n",
    "# technically removes all characters that are not letter, numbers, punctuation, or whitespace\n",
    "no_emojis = []\n",
    "for tweet in non_links:\n",
    "    for c in list(tweet):\n",
    "        if not c.isalnum() and c not in string.punctuation and c not in string.whitespace:\n",
    "            tweet = tweet.replace(c, '')\n",
    "    no_emojis.append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# changes '&amp;' to simply '&'\n",
    "# NEW - maybe replace with 'and' so it will learn how to spell 'and' rather have to learn where to place &\n",
    "# idk bru\n",
    "final_tweets = []\n",
    "for tweet in no_emojis:\n",
    "    if '&amp;' in tweet:\n",
    "        tweet = tweet.replace('&amp;', '&')\n",
    "    final_tweets.append(tweet)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2983"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.@FLOTUS & I were honored to host our first WH Congressional Picnic. A wonderful evening & tradition. @MarineBand:'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_tweets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".@FLOTUS &amp; I were honored to host our first WH Congressional Picnic. A wonderful evening &amp; tradition. @MarineBand:â€¦ https://t.co/h5L4myWmam\n",
      ".@FLOTUS & I were honored to host our first WH Congressional Picnic. A wonderful evening & tradition. @MarineBand:\n"
     ]
    }
   ],
   "source": [
    "print(non_retweets[0])\n",
    "print(final_tweets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets:  2983\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of tweets: \", len(final_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars: 95\n"
     ]
    }
   ],
   "source": [
    "# all code below is based on Keras example documentation for LSTM Text Generation\n",
    "chars = sorted(list(set(' '.join(final_tweets))))\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest tweet in characters:  141\n",
      "Shortest tweet in characters:  10\n"
     ]
    }
   ],
   "source": [
    "max_len = max(map(len, [tweet for tweet in final_tweets]))\n",
    "print(\"Longest tweet in characters: \", max_len)\n",
    "\n",
    "min_len = min(map(len, [tweet for tweet in final_tweets]))\n",
    "print(\"Shortest tweet in characters: \", min_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7ff067fb0240>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY8AAAEWCAYAAACe8xtsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGR6SpN4MD0lSb4aHJKm3/w+k7pgvarO16gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff03c129dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "lengths = []\n",
    "for tweet in final_tweets:\n",
    "    lengths.append(len(tweet))\n",
    "\n",
    "bin_width = 20\n",
    "plt.hist(lengths, bins=range(min(lengths), max(lengths)+bin_width, bin_width))\n",
    "plt.title(\"Lengths of tweets\")\n",
    "plt.xlabel(\"Length\")\n",
    "plt.ylabel(\"Frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 95719\n"
     ]
    }
   ],
   "source": [
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "# idea for later cut text relative to size of tweet aka maxlen = len(tweet)/3\n",
    "maxlen = 10\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for tweet in final_tweets:\n",
    "    if len(tweet) > maxlen:\n",
    "        for i in range(0, len(tweet) - maxlen, step):\n",
    "            sentences.append(tweet[i: i + maxlen])\n",
    "            next_chars.append(tweet[i + maxlen])\n",
    "    # two options here:\n",
    "    # could iterate with a range like above, using lengths of 10 (min tweet length)\n",
    "    # or could just get one sentence from each tweet based on length of each tweet\n",
    "    else:\n",
    "        sentences.append(tweet[:len(tweet)-2])\n",
    "        next_chars.append(tweet[len(tweet)-1])\n",
    "print('nb sequences:', len(sentences))\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# might need to pad shorter sentences? jk apparently not\n",
    "print('Vectorization...')\n",
    "X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# might need to pad shorter sentences? jk apparently not\n",
    "print('Vectorization...')\n",
    "X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "# Building the model\n",
    "print('Building model...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars)))) # , return_sequences=True)) # might have problems if fed a shorter tweet than maxlen?\n",
    "model.add(Dropout(0.2))\n",
    "# model.add(LSTM(128))\n",
    "# model.add(Dropout(0.2))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "optimizer = RMSprop(lr=0.01) # experiment by adding ReduceLROnPlateau\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# essentially a softmax function with an option to change the degree of variability\n",
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# actually don't think I need this, just starting with a different X and y\n",
    "from keras.models import load_model\n",
    "\n",
    "# RELOAD BEST ONE LAYER AND JUST TRAIN IT MORE while finishing up\n",
    "model = load_model('my_model_BEST_VERSION.h5') # start here and reload. You just saved best one layer version. Currently running first two layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=3, min_lr=0.0001, verbose=1, epsilon=0.0003)\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=5)\n",
    "checkpointer = ModelCheckpoint(filepath='new_model.h5', verbose=1, save_best_only=True)\n",
    "\n",
    "model.fit(X, y, batch_size=128, epochs=500, callbacks=[reduce_lr, early_stopping, checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "67600/67600 [==============================] - 355s - loss: 6.7670   \n",
      "Epoch 2/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hbh3re/anaconda3/lib/python3.6/site-packages/keras/callbacks.py:405: RuntimeWarning: Can save best model only with val_loss available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67600/67600 [==============================] - 339s - loss: 6.5344   \n",
      "Epoch 3/500\n",
      "67600/67600 [==============================] - 376s - loss: 6.3068   \n",
      "Epoch 4/500\n",
      "67600/67600 [==============================] - 322s - loss: 6.1378   \n",
      "Epoch 5/500\n",
      "67600/67600 [==============================] - 402s - loss: 5.9845   \n",
      "Epoch 6/500\n",
      "67600/67600 [==============================] - 330s - loss: 5.9072   \n",
      "Epoch 7/500\n",
      "67600/67600 [==============================] - 324s - loss: 5.8071   \n",
      "Epoch 8/500\n",
      "67600/67600 [==============================] - 379s - loss: 5.7133   \n",
      "Epoch 9/500\n",
      "67600/67600 [==============================] - 353s - loss: 5.6145   \n",
      "Epoch 10/500\n",
      "67600/67600 [==============================] - 379s - loss: 5.5624   \n",
      "Epoch 11/500\n",
      "67600/67600 [==============================] - 349s - loss: 5.4516   \n",
      "Epoch 12/500\n",
      "67600/67600 [==============================] - 327s - loss: 5.3945   \n",
      "Epoch 13/500\n",
      "67600/67600 [==============================] - 370s - loss: 5.3883   \n",
      "Epoch 14/500\n",
      "67600/67600 [==============================] - 335s - loss: 5.3400   \n",
      "Epoch 15/500\n",
      "67600/67600 [==============================] - 385s - loss: 5.3218   \n",
      "Epoch 16/500\n",
      "67600/67600 [==============================] - 351s - loss: 5.2863   \n",
      "Epoch 17/500\n",
      "67600/67600 [==============================] - 301s - loss: 5.2617   \n",
      "Epoch 18/500\n",
      "67600/67600 [==============================] - 346s - loss: 5.2471   \n",
      "Epoch 19/500\n",
      "67600/67600 [==============================] - 334s - loss: 5.1946   \n",
      "Epoch 20/500\n",
      "67600/67600 [==============================] - 312s - loss: 5.1543   \n",
      "Epoch 21/500\n",
      "67600/67600 [==============================] - 360s - loss: 5.1387   \n",
      "Epoch 22/500\n",
      "67600/67600 [==============================] - 304s - loss: 5.1058   \n",
      "Epoch 23/500\n",
      "67600/67600 [==============================] - 322s - loss: 5.0701   \n",
      "Epoch 24/500\n",
      "19840/67600 [=======>......................] - ETA: 246s - loss: 5.0557"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=3, min_lr=0.0001, verbose=1, epsilon=0.0003)\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=5)\n",
    "checkpointer = ModelCheckpoint(filepath='new_model.h5', verbose=1, save_best_only=True)\n",
    "\n",
    "model.fit(X, y, batch_size=128, epochs=500, callbacks=[reduce_lr, early_stopping, checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "model.save('new_model_maxlen_10_epoch_35.h5') # we are currenlty running maxlen = 40, batch_size = 64, two LSTMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "lengths2 = []\n",
    "for sent in sentences:\n",
    "    lengths2.append(len(list(sent)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 40}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listy = set(lengths2)\n",
    "listy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hbh3re/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: RuntimeWarning: divide by zero encountered in log\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2\n",
      "Iteration 3\n",
      "Iteration 4\n",
      "Iteration 5\n",
      "Iteration 6\n",
      "Iteration 7\n",
      "Iteration 8\n",
      "Iteration 9\n",
      "Iteration 10\n",
      "Iteration 11\n",
      "Iteration 12\n",
      "Iteration 13\n",
      "Iteration 14\n",
      "Iteration 15\n",
      "Iteration 16\n",
      "Iteration 17\n",
      "Iteration 18\n",
      "Iteration 19\n",
      "Iteration 20\n",
      "Iteration 21\n",
      "Iteration 22\n",
      "Iteration 23\n",
      "Iteration 24\n",
      "Iteration 25\n",
      "Iteration 26\n",
      "Iteration 27\n",
      "Iteration 28\n",
      "Iteration 29\n",
      "Iteration 30\n",
      "Iteration 31\n",
      "Iteration 32\n",
      "Iteration 33\n",
      "Iteration 34\n",
      "Iteration 35\n",
      "Iteration 36\n",
      "Iteration 37\n",
      "Iteration 38\n",
      "Iteration 39\n",
      "Iteration 40\n",
      "Iteration 41\n",
      "Iteration 42\n",
      "Iteration 43\n",
      "Iteration 44\n",
      "Iteration 45\n",
      "Iteration 46\n",
      "Iteration 47\n",
      "Iteration 48\n",
      "Iteration 49\n",
      "Iteration 50\n",
      "Iteration 51\n",
      "Iteration 52\n",
      "Iteration 53\n",
      "Iteration 54\n",
      "Iteration 55\n",
      "Iteration 56\n",
      "Iteration 57\n",
      "Iteration 58\n",
      "Iteration 59\n",
      "Iteration 60\n",
      "Iteration 61\n",
      "Iteration 62\n",
      "Iteration 63\n",
      "Iteration 64\n",
      "Iteration 65\n",
      "Iteration 66\n",
      "Iteration 67\n",
      "Iteration 68\n",
      "Iteration 69\n",
      "Iteration 70\n",
      "Iteration 71\n",
      "Iteration 72\n",
      "Iteration 73\n",
      "Iteration 74\n",
      "Iteration 75\n",
      "Iteration 76\n",
      "Iteration 77\n",
      "Iteration 78\n",
      "Iteration 79\n",
      "Iteration 80\n",
      "Iteration 81\n",
      "Iteration 82\n",
      "Iteration 83\n",
      "Iteration 84\n",
      "Iteration 85\n",
      "Iteration 86\n",
      "Iteration 87\n",
      "Iteration 88\n",
      "Iteration 89\n",
      "Iteration 90\n",
      "Iteration 91\n",
      "Iteration 92\n",
      "Iteration 93\n",
      "Iteration 94\n",
      "Iteration 95\n",
      "Iteration 96\n",
      "Iteration 97\n",
      "Iteration 98\n",
      "Iteration 99\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import sys\n",
    "\n",
    "list_1 = []\n",
    "list_05 = []\n",
    "list_02 = []\n",
    "# train the model, output generated text after each iteration\n",
    "for iteration in range(1, 100):\n",
    "    # print()\n",
    "    # print('-' * 50)\n",
    "    print('Iteration', iteration)\n",
    "    '''model.fit(X, y,\n",
    "              batch_size=128,\n",
    "              epochs=1)'''\n",
    "\n",
    "    start_index = random.randint(0, len(sentences)) # choose index to get random tweet beginning\n",
    "\n",
    "    for diversity in [0.2, 0.5, 1.0]:\n",
    "        # print()\n",
    "        # print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = sentences[start_index] # get the tweet using the random index\n",
    "        generated += sentence\n",
    "        # print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        # sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(140-maxlen): # max length of a tweet is 140 characters\n",
    "            x = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            # sys.stdout.write(next_char)\n",
    "            # sys.stdout.flush()\n",
    "        # print()\n",
    "        if diversity == 0.2:\n",
    "            list_02.append(generated)\n",
    "        if diversity == 0.5:\n",
    "            list_05.append(generated)\n",
    "        if diversity == 1.0:\n",
    "            list_1.append(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# List of words in a sentence to all ngrams\n",
    "def ngrams(words, n):\n",
    "    words = [w.strip().lower() for w in words.split()]\n",
    "    return list(zip(*[words[i:] for i in range(n)]))\n",
    "\n",
    "# SpaCy sentence to words in sentence as strings\n",
    "def get_words(sentence):\n",
    "    return list(w for w in sentence.split() if w.isalpha())\n",
    "\n",
    "# Get all ngrams from a SpaCy corpus\n",
    "def get_request_ngrams(list_, n):\n",
    "    return [gram for sent in list_ for gram in ngrams(get_words(sent), n)]\n",
    "\n",
    "# Get all ngrams from a list of SpaCy corpora\n",
    "def get_all_ngrams(request_text, n):\n",
    "    grams = set()\n",
    "    for text in request_text:\n",
    "        grams.update(get_request_ngrams(text, n))\n",
    "    return grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "trigram02 = []\n",
    "for tweet in list_02:\n",
    "    trigram02.append(ngrams(tweet, 3))\n",
    "    \n",
    "trigram05 = []\n",
    "for tweet in list_05:\n",
    "    trigram05.append(ngrams(tweet, 3))\n",
    "    \n",
    "trigram1 = []\n",
    "for tweet in list_1:\n",
    "    trigram1.append(ngrams(tweet, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "flat_list02 = [item for sublist in trigram02 for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['up--and whe after helling have a borery that me would but illegal immigration and bad and have a bettle the proud of my learing and was so m',\n",
       " 'ff for Cincinnati, Ohio to get me would but when I will be in the China on @FoxNews at the last night at the last night at the last night at',\n",
       " 'othing about the world. We will be there was a total stronghey at ANR A ALL  REATON ANE AN Ackon Amaiefiom to be have a big respect to the g',\n",
       " ' to cost you to the disgrace to be prodesd a story bust and the so want to sades stand have a big respect to the great people workd atteal t',\n",
       " 'y has ever before the prodesd states and going to be a great worked batters! #DrainTheSwamp #Americabifs! #ImWithYou #Trump2016 #AmericaFirs']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_02[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"aine oversaties amaigctions to is dead to see it will make she didn't thempen! #MAGA Tickets: #ImWithYou #AmericaFirst #MAGA! Thank you for \",\n",
       " 'people who are beane and the media will going to have a line that my failing @nytimes story going to inficering for their never she is a lon',\n",
       " \"lDonaldTrump @Senatianation a total stronghey plan on the time in her I am should be great calls would be calling that she will don't be the\",\n",
       " 'le, North Carolina that she will destroy some out! #MAGA Tickets: #DrainTheSwamp #AmericaFirst #MAGA! Toge haned is running again! #DrainThe',\n",
       " 'The White House to she is a disaster! When harling to be bad the beginning to the hange in the United States and get in the media is a creat']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_05[5:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['interest with she will drigat State email on my Truwp Io, IowE ! Insidealtimes! Enj ythes are crease on our joblity. The lations, the Writio',\n",
       " 'amond and  course yeve Is action plan and defend to put wall thing to do sholling my really believe the @2gswexfast. DClonpenday to get out ',\n",
       " 'asp when the Democrats Kan Averina Respented that will deneers and proke wath to all of the Democrats, jobs in the continumed by the United ',\n",
       " 'is in a dex me to we I disanairs. But racly millions! Numanisthachs to bail to but a bad ! Biggtam Carolina tomorrow at nManS and 8dmn. Fake',\n",
       " 'ookedHillary is the udObamaning Obama is Governor @Mike_Pence ! Fine @mighetearion should take a want of the Dems are to believe the WILL .o']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_1[10:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('will', 'be', 'there'), 21),\n",
       " (('the', 'failing', '@nytimes'), 20),\n",
       " (('the', 'last', 'night'), 19),\n",
       " (('media', 'is', 'a'), 19),\n",
       " (('last', 'night', 'at'), 18),\n",
       " (('be', 'there', 'was'), 18),\n",
       " (('the', 'media', 'is'), 18),\n",
       " (('night', 'at', 'the'), 15),\n",
       " (('at', 'the', 'last'), 13),\n",
       " (('to', 'be', 'a'), 13)]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def Most_Common(lst):\n",
    "    data = Counter(lst)\n",
    "    return data.most_common(10)\n",
    "\n",
    "Most_Common(flat_list02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gram1 = []\n",
    "for tweet in list_1:\n",
    "    gram1.append(ngrams(tweet, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags = [item for sublist in gram1 for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags2 = [tag for tag in hashtags if tag[0][0] == '#']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('#debate',), 5),\n",
       " (('#americafirst!',), 3),\n",
       " (('#draintheswamp',), 3),\n",
       " (('#trump2016',), 3),\n",
       " (('#maga',), 2),\n",
       " (('#imwithyou',), 2),\n",
       " (('#imw',), 1),\n",
       " (('#americafirst',), 1),\n",
       " (('#makeamericagreatagain.',), 1),\n",
       " (('#magae',), 1)]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Most_Common(hashtags2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What I will do in the future:\n",
    "\n",
    "- Experiement more with the model\n",
    "- Pin down which length is best to train on\n",
    "- Main goal: add a spell checking network on top of the text generation network\n",
    "- Perhaps add sentiment analysis and similarity between words, and combining with those results during training\n",
    "- Add credits for functions used above"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
